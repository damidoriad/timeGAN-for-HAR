{"cells":[{"cell_type":"markdown","metadata":{"id":"1MVttu53jcJE"},"source":["# timeGAN para HAR\n","\n","- A implementação original, disponível [aqui](https://github.com/jsyoon0823/TimeGAN/tree/master), usou o TensorFlow v1.\n","- Uma implementação alternativa usando TensorFlow v2 (com alguns erros) está disponível [aqui](https://www.kaggle.com/code/alincijov/stocks-generate-synthetic-data-timegan).\n","- Uma implementação usando pytorch foi desenvolvida em 2021 e está disponível [aqui](https://github.com/benearnthof/TimeGAN/tree/main).\n","- Uma outra implementação usando pytorch está disponível [aqui](https://github.com/zzw-zwzhang/TimeGAN-pytorch/tree/main)\n","\n","A proposta aqui é tentar adaptar o modelo original para dados de sensores inerciais em tarefas de HAR."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch \n","from torch import nn\n","from torch import functional as F\n","import os\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from torchinfo import summary"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["file = 'stock_data.csv'\n","raw = np.loadtxt(file, delimiter = \",\",skiprows = 1)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def Normalize(dta):\n","  return (dta - np.min(dta, 0)) /  (np.max(dta, 0) - np.min(dta, 0) + 1e-7)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def real_data_loading (path, seq_len):\n","  \"\"\"Load and preprocess real-world datasets.\n","  \n","  Args:\n","    - data_name: stock or energy\n","    - seq_len: sequence length\n","    \n","  Returns:\n","    - data: preprocessed data.\n","  \"\"\"  \n","  assert os.path.isfile(path)\n","  ori_data = np.loadtxt(path, delimiter = \",\",skiprows = 1)\n","        \n","  # Flip the data to make chronological data\n","  ori_data = ori_data[::-1]\n","  # Normalize the data\n","  ori_data = Normalize(ori_data)\n","    \n","  # Preprocess the dataset\n","  temp_data = []    \n","  # Cut data by sequence length\n","  for i in range(0, len(ori_data) - seq_len):\n","    _x = ori_data[i:i + seq_len]\n","    # yields an array of dims [len(dta) - seq_len, seq_len, n_variables]\n","    temp_data.append(_x)\n","        \n","  # Mix the datasets (to make it similar to i.i.d)\n","  idx = np.random.permutation(len(temp_data))    \n","  data = []\n","  for i in range(len(temp_data)):\n","    data.append(temp_data[idx[i]])\n","    \n","  return data"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["seq_len = 24\n","ori_data = real_data_loading(file, seq_len)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def train_test_divide (data_x, data_x_hat, data_t, data_t_hat, train_rate = 0.8):\n","  \"\"\"Divide train and test data for both original and synthetic data.\n","  \n","  Args:\n","    - data_x: original data\n","    - data_x_hat: generated data\n","    - data_t: original time\n","    - data_t_hat: generated time\n","    - train_rate: ratio of training data from the original data\n","  \"\"\"\n","  # Divide train/test index (original data)\n","  no = len(data_x)\n","  idx = np.random.permutation(no)\n","  train_idx = idx[:int(no*train_rate)]\n","  test_idx = idx[int(no*train_rate):]\n","    \n","  train_x = [data_x[i] for i in train_idx]\n","  test_x = [data_x[i] for i in test_idx]\n","  train_t = [data_t[i] for i in train_idx]\n","  test_t = [data_t[i] for i in test_idx]      \n","    \n","  # Divide train/test index (synthetic data)\n","  no = len(data_x_hat)\n","  idx = np.random.permutation(no)\n","  train_idx = idx[:int(no*train_rate)]\n","  test_idx = idx[int(no*train_rate):]\n","  \n","  train_x_hat = [data_x_hat[i] for i in train_idx]\n","  test_x_hat = [data_x_hat[i] for i in test_idx]\n","  train_t_hat = [data_t_hat[i] for i in train_idx]\n","  test_t_hat = [data_t_hat[i] for i in test_idx]\n","  \n","  return train_x, train_x_hat, test_x, test_x_hat, train_t, train_t_hat, test_t, test_t_hat"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def extract_time (data):\n","  \"\"\"Returns Maximum sequence length and each sequence length.\n","  \n","  Args:\n","    - data: original data\n","    \n","  Returns:\n","    - time: extracted time information\n","    - max_seq_len: maximum sequence length\n","  \"\"\"\n","  time = list()\n","  max_seq_len = 0\n","  for i in range(len(data)):\n","    max_seq_len = max(max_seq_len, len(data[i][:,0]))\n","    time.append(len(data[i][:,0]))\n","    \n","  return time, max_seq_len"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["seq_lengths, max_seq_len = extract_time(ori_data)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def batch_generator(data, time, batch_size):\n","  \"\"\"Mini-batch generator.\n","  \n","  Args:\n","    - data: time-series data\n","    - time: time information\n","    - batch_size: the number of samples in each batch\n","    \n","  Returns:\n","    - X_mb: time-series data in each batch\n","    - T_mb: time information in each batch\n","  \"\"\"\n","  no = len(data)\n","  idx = np.random.permutation(no)\n","  train_idx = idx[:batch_size]     \n","            \n","  X_mb = list(data[i] for i in train_idx)\n","  T_mb = list(time[i] for i in train_idx)\n","  \n","  return X_mb, T_mb"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["ori_time, max_seq_len = extract_time(ori_data)\n","no, seq_len, dim = np.asarray(ori_data).shape\n","batch_size = 128\n","z_dim = dim"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["def random_generator (batch_size, z_dim, T_mb, max_seq_len):\n","  \"\"\"Random vector generation.\n","  \n","  Args:\n","    - batch_size: size of the random vector\n","    - z_dim: dimension of random vector\n","    - T_mb: time information for the random vector\n","    - max_seq_len: maximum sequence length\n","    \n","  Returns:\n","    - Z_mb: generated random vector\n","  \"\"\"\n","  Z_mb = list()\n","  for i in range(batch_size):\n","    temp = np.zeros([max_seq_len, z_dim])\n","    temp_Z = np.random.uniform(0., 1, [T_mb[i], z_dim])\n","    temp[:T_mb[i],:] = temp_Z\n","    Z_mb.append(temp_Z)\n","  return Z_mb"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size) "]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["(128, 24, 6)"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n","np.array(Z_mb).shape"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# X = tf.placeholder(tf.float32, [None, max_seq_len, dim], name = \"myinput_x\")\n","# Z = tf.placeholder(tf.float32, [None, max_seq_len, z_dim], name = \"myinput_z\")\n","# T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")\n","# X is the original data => Flexible batch size, 24, 6 for stock data\n","# Z is the generated fake data => Flex batch size, 24, 6 for stock data\n","# T is the time information => Flexible batch size, here 128 for the time data\n","# the embedding network uses \n","num_layers = 2\n","hidden_dim = 24\n","seq_len = 24\n","# i think input size should be 24 for the stock data\n","input_size = 6\n","device = \"cpu\"\n","batch_size = 128\n","\n","# from the RNN documentation: \n","# arguments: inputsize = num_features, hidden_size, num_layers\n","# rnn = nn.RNN(6, 24, 2, batch_first=True)\n","# batch seq features 128, 24, 6\n","# input: batch, length, hidden\n","# input = torch.randn(128, 24, 6)\n","# num layers, batch, hidden size\n","# h0 = torch.randn(2, 128, 24)\n","# output, hn = rnn(input, h0)\n","\n","class RNN(nn.Module):\n","  def __init__(self, input_size, hidden_dim, num_layers):\n","    super(RNN, self).__init__()\n","    self.num_layers = num_layers\n","    self.hidden_dim = hidden_dim\n","    # batch must be first dimension, inputsize = 6 for tme data\n","    # arguments: inputsize = num_features, hidden_size, num_layers\n","    # rnn = nn.RNN(6, 24, 2, batch_first=True)\n","    self.rnn = nn.RNN(input_size, hidden_dim, num_layers, batch_first = True)\n","    # X => batch_size, seq_length, num_features like specified above\n","    self.fc = nn.Linear(hidden_dim, hidden_dim)\n","    self.nonlinearity = nn.Sigmoid()\n","\n","  # rnn needs two inputs: data & initial state\n","  def forward(self, x):\n","    # num layers, batch, hidden size\n","    # h0 = torch.randn(2, 128, 24)\n","    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device).float()\n","    out, hn = self.rnn(x, h0)\n","    # out: batch_size, seq_len, hidden_dim\n","    # out = out[:, -1, :]\n","    # out (128, 24)\n","    out = self.fc(out)\n","    out = self.nonlinearity(out)\n","    return out"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# testing the RNN module\n","net = RNN(6, 24, 5)\n","# converting everything to float to avoid data type runtime errors\n","net.float()\n","\n","# using one minibatch as example data\n","dta = np.array(X_mb)\n","dta.astype(np.float32)\n","dta = torch.from_numpy(dta)\n","\n","emb = net(dta.float())\n","# yields 128 x 24 embeddings => 128 samples in batch, every embedding has 24 features\n","# => we embed in a higher dimensional space in this case, as mentioned in the paper"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([24, 24])"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["emb[0].shape"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","RNN                                      [128, 24, 24]             --\n","├─RNN: 1-1                               [128, 24, 24]             5,568\n","├─Linear: 1-2                            [128, 24, 24]             600\n","├─Sigmoid: 1-3                           [128, 24, 24]             --\n","==========================================================================================\n","Total params: 6,168\n","Trainable params: 6,168\n","Non-trainable params: 0\n","Total mult-adds (Units.MEGABYTES): 17.18\n","==========================================================================================\n","Input size (MB): 0.07\n","Forward/backward pass size (MB): 1.18\n","Params size (MB): 0.02\n","Estimated Total Size (MB): 1.28\n","=========================================================================================="]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["summary(net, input_size=(batch_size, 24, 6))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPJTMaMaQcag3lHA2PBrSfN","collapsed_sections":["1MVttu53jcJE","t_wk8HxcVpKf"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
